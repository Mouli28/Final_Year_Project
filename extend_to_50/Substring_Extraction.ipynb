{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20719926-3fd7-4931-b961-575f0e34040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"Bracket AD1066\"\n",
    "    df, urls = price_comparison(query, num_results=50)\n",
    "    file_name = f\"{query.replace(' ', '_')}_nlp_part_details.xlsx\"\n",
    "    df.to_excel(file_name, index=False)\n",
    "    wb = load_workbook(file_name)\n",
    "    ws = wb.active\n",
    "    for row in ws.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, horizontal='left', vertical = 'top')\n",
    "    wb.save(file_name)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b96e788c-7b6a-45e8-951b-216a06c07326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Function to extract Google search URLs\n",
    "def get_google_search_urls(query, num_results):\n",
    "    \"\"\"\n",
    "    Retrieves URLs from Google search results for the given query.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query.\n",
    "    - num_results (int): Number of search results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of URLs extracted from the search results.\n",
    "    \"\"\"\n",
    "    search_urls = []\n",
    "    options = Options()\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(\"https://www.google.com\")\n",
    "\n",
    "    # Locate the search box, enter the query, and perform the search\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Wait for results to load\n",
    "    driver.implicitly_wait(2)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    # Extract URLs from the first page\n",
    "    results = driver.find_elements(By.CSS_SELECTOR, \"div.yuRUbf a\")\n",
    "    urls.extend([result.get_attribute(\"href\") for result in results])\n",
    "\n",
    "    # If less than the required number of results, navigate to the next page\n",
    "    while len(urls) < num_results:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.ID, \"pnnext\")\n",
    "            next_button.click()\n",
    "            driver.implicitly_wait(2)\n",
    "            results = driver.find_elements(By.CSS_SELECTOR, \"div.yuRUbf a\")\n",
    "            urls.extend([result.get_attribute(\"href\") for result in results])\n",
    "        except Exception as e:\n",
    "            print(f\"Error navigating to the next page: {e}\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Return only the required number of URLs\n",
    "    return urls[:num_results]\n",
    "\n",
    "\n",
    "# Function to extract product details from a specific webpage\n",
    "def extract_product_details(url, substrings):\n",
    "    \"\"\"\n",
    "    Extracts product details from the given URL by dynamically identifying\n",
    "    class names containing specific substrings and extracting all instances\n",
    "    of data for each matched class.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the product page.\n",
    "    - substrings (dict): A dictionary where keys are the data fields (e.g., 'name', 'price')\n",
    "                         and values are lists of substrings to search for in class names.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the extracted product details.\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store product details\n",
    "    product_details = {}\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait until the page is fully loaded\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        page_source = driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return product_details\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract all class names\n",
    "    all_elements = soup.find_all(True, class_=True)\n",
    "    class_names = set()\n",
    "    for element in all_elements:\n",
    "        classes = element.get('class')\n",
    "        for cls in classes:\n",
    "            class_names.add(cls)\n",
    "\n",
    "    # For each data field, find all class names that contain any of the specified substrings\n",
    "    for field, substr_list in substrings.items():\n",
    "        matched_classes = []\n",
    "        for cls in class_names:\n",
    "            for substr in substr_list:\n",
    "                if substr.lower() in cls.lower():\n",
    "                    matched_classes.append(cls)\n",
    "                    break\n",
    "\n",
    "        # If we found matched classes, extract data from all instances\n",
    "        if matched_classes:\n",
    "            extracted_data = []\n",
    "            for matched_class in matched_classes:\n",
    "                elements = soup.find_all(class_=re.compile(matched_class))\n",
    "                for element in elements:\n",
    "                    extracted_data.append(element.get_text(strip=True))\n",
    "            # Store all instances in a list, joining them as a single string for display\n",
    "            product_details[field] = '; '.join(extracted_data)\n",
    "        else:\n",
    "            product_details[field] = None\n",
    "\n",
    "    return product_details\n",
    "\n",
    "# Function to perform price comparison using extracted product details\n",
    "def price_comparison(query, num_results):\n",
    "    \"\"\"\n",
    "    Performs price comparison for the given query by extracting product details from multiple websites.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query (e.g., \"AD1066 bracket\").\n",
    "    - num_results (int): Number of Google search results to process.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A pandas DataFrame containing the product details and a list of URLs processed.\n",
    "    \"\"\"\n",
    "    urls = get_google_search_urls(query, num_results)\n",
    "    all_product_details = []\n",
    "    processed_urls = []\n",
    "\n",
    "    # Define substrings for dynamic class name matching\n",
    "    substrings = {\n",
    "        'Product name': ['productName', 'product-title', 'mainTitle', 'product-detail-title', 'productTitle', 'titleSection'],\n",
    "        'price': ['corePriceDisplay_desktop_feature_div', 'price', 'msrp', 'product-price', 'x-price-primary'],\n",
    "        'description': ['vim d-item-description','description','prodDetails', 'ProductDetails', 'item-desc isColorImage', 'product_description_wrapper', 'productDetails_techSpec_section_1', 'x-item-description-child', 'product-details', 'product_info_description_list','d-item-description','product-details-inner', 'description-collapse', 'description','product-details-module'],\n",
    "        'Taxonomy' : ['breadcrumb', 'bread-crumb'],\n",
    "        'Part No': ['part_number', 'partNumSection', 'part-number', 'product_part-info', 'item-part-number'],\n",
    "        'Cross Reference':['Crossreference','replaces', 'Interchange', 'product superseded','superseded','x-item-description-child'],\n",
    "        'specifications': ['spec', 'specs', 'details', 'product-spec','specification-collapse'],\n",
    "        'warranty': ['warranty', 'guarantee'],\n",
    "        'availability': ['availability', 'stock', 'in-stock']\n",
    "    }\n",
    "\n",
    "    for url in urls:\n",
    "        details = extract_product_details(url, substrings)\n",
    "        if details:\n",
    "            details['url'] = url  # Optionally add the URL to the details\n",
    "            all_product_details.append(details)\n",
    "            processed_urls.append(url)\n",
    "\n",
    "    df = pd.DataFrame(all_product_details)\n",
    "    return df, processed_urls\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    location = filelocation here #file location here\n",
    "    list = pd.read_excel(location)\n",
    "    combined_list = list.apply(lambda row: f\"{row['Description']} {row['Part Number']}\", axis=1).tolist()\n",
    "    for query in combined_list:\n",
    "        df, urls = price_comparison(query, num_results=50)\n",
    "        file_name = f\"{query.replace(' ', '_')}_nlp_part_details.xlsx\"\n",
    "        df.to_excel(file_name, index=False)\n",
    "        wb = load_workbook(file_name)\n",
    "        ws = wb.active\n",
    "        for row in ws.iter_rows():\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(wrap_text=True, horizontal='left', vertical='top')\n",
    "        wb.save(file_name)\n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f5aaa-7914-48b9-bd4b-34aed600e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
